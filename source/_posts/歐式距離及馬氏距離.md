---
title: 歐式距離及馬氏距離
date: 2017-10-27 17:15:24
tags: [statistics,math,machine learning,data analysis]
categories: Statistics
thumbnail: /2017/10/27/歐式距離及馬氏距離/mahalanobis.PNG
mathjax: false
---

今天在分析產學案的資料想試試看馬氏距離，但用了Matlab內建的函式怎樣算出來都不太符合預期，於是重新理解了一遍馬氏距離，這邊做一些筆記。

這篇將會以我本人的理解思路來進行，不會全面的解釋資料間距離的計算方式，如果想了解各種距離的計算方法跟公式可參考以下這篇。

Ref: [机器学习中的相似性度量](http://www.cnblogs.com/heaad/archive/2011/03/08/1977733.html)

在統計學上我們會想要分析某兩組資料的相似程度，其中一種方法是計算兩組資料的「距離(distance)」，有很多的方法可以計算距離，在開始計算距離之前先理解一件事：何謂資料的「維度(dimension)」

在高中我們第一次聽到維度這個詞應該是在學座標系統時，在座標系統裡維度的意思是「在一個空間裡面最少需要幾個座標軸來表達一個點」，而資料的維度是指一筆資料有多少個不同的變數，也就是有多少個欄位的意思，例如說你今天有一組班上的通訊錄資料，每一筆資料都包含每個人的「身高、體重、年齡、出生地、入學考平均」，一共5個欄位，表示你手上的通訊錄資料的維度是5。

統計上的維度其實也可以和幾何學上的維度做類比，你可以把每一筆資料想像成是空間中一個點，欄位就是空間中的座標軸，那麼要表達一個點(一筆資料)所需要的座標軸數量(欄位數量)就是維度。

資料的維度會影響到統計分析的複雜性，因此有時候會聽到在研究降低資料維度的方法，意思就是在想辦法降低資料的欄位數目，同時保持原本的資料特性，來降低分析的複雜度。

有了維度的概念後可以開始來談距離了。

## 歐式距離(Euclidean Distance)

在幾何空間中我們在高中就有學過如何計算兩個點之間的距離，以二維平面來說，我們在高中時所用的方法為，計算兩個點之間x和y各自的差距，並使用畢氏定理來計算兩點間的距離，三維空間可以用一樣的方法來計算，公式如下

{% raw %}
$$
d = \sqrt{(x_2-x_1)^2 + (y_2-y_1)^2 + (z_2-z_1)^2}
$$
{% endraw %}

這個方法可以推廣到更高的維度上

{% raw %}
$$
d = \sqrt{\sum_{i=1}^{n} (x_{2i}-x_{1i})^2}
$$
{% endraw %}

其中n表示最高維度

我們也可用相同的方法計算兩筆資料之間的距離，因為適來自歐式座標的距離計算，所以這個方法稱為歐式距離。

但歐式距離的假設是所有維度的尺度是一致的，沒有單位的概念；在空間座標中，每個座標軸上距離1是等價的，所以我們把兩個點在各個座標軸的距離合併在一起看是沒問題的，但對於資料來講可不一定，不同欄位有不同的單位，我們不可能說1公斤和1公尺是等價的，若我們將空間座標加入單位的概念也會發生一樣的問題，如果X軸的單位是公分而Y軸的單位是公尺，顯然相同的距離差距在X軸上的數字會較大，造成X軸的距離差和Y軸的距離差對整體距離的貢獻度不同，但實際上應該是要相近的。

## 標準化歐式距離(Standardized Euclidean distance)

將資料去除單位的方法之一是標準化，我們可以先將各個維度的資料進行標準化之後再來計算距離，這就是標準化歐式距離

對於維度i的標準化

{% raw %}
$$
Z_i = \frac{X_i-\mu_i}{\sigma_i}
$$
{% endraw %}

將所有維度標準化後計算標準化資料之間的歐式距離

{% raw %}
$$
d = \sqrt{\sum_{i=1}^{n} (z_{2i}-z_{1i})^2}
$$
{% endraw %}

將標準化的公式帶入可推導出

{% raw %}
$$
d = \sqrt{\sum_{i=1}^{n} (\frac{x_{2i}-x_{1i}}{\sigma_i})^2}
$$
{% endraw %}

這成功解決了因為尺度不同造成不同維度對距離計算有不同貢獻的問題。

除此之外歐式距離仍存在另一個假設：不同維度之間是獨立的。

什麼意思呢？試想身高體重的例子，雖然身高和體重沒有直接關係，但身高和體重之間應該呈現微弱的正相關(畢竟高的人比矮的人多長一些骨頭，應該要比較重一些)，也就是說體重的資訊裡面可能有包含身高的部份資訊，我們需要再更進一步考慮資料相關性的問題。

## 馬氏距離(Mahalanobis Distance)

兩組資料間(這裡的兩組指的是不同維度)的相關性可以用共變異數矩陣來表達：

{% raw %}
$$
S = \begin{bmatrix}
 \mathrm{E}[(X_1 - \mu_1)(X_1 - \mu_1)] & \mathrm{E}[(X_1 - \mu_1)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_1 - \mu_1)(X_n - \mu_n)] \\ \\
 \mathrm{E}[(X_2 - \mu_2)(X_1 - \mu_1)] & \mathrm{E}[(X_2 - \mu_2)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_2 - \mu_2)(X_n - \mu_n)] \\ \\
 \vdots & \vdots & \ddots & \vdots \\ \\
 \mathrm{E}[(X_n - \mu_n)(X_1 - \mu_1)] & \mathrm{E}[(X_n - \mu_n)(X_2 - \mu_2)] & \cdots & \mathrm{E}[(X_n - \mu_n)(X_n - \mu_n)]
\end{bmatrix}
$$
{% endraw %}

馬氏距離的概念我們可以想像成是將座標軸縮放並重劃，把各個維度之間的相依性去除後再計算距離，公式如下

{% raw %}
$$
d(\vec{x},\vec{y}) = \sqrt{(\vec{x}-\vec{y})^T S^{-1} (\vec{x}-\vec{y})}
$$
{% endraw %}

要注意這個公式以向量的形式表示，記得我一開始有提到我們可以把每一組資料視為一個在N維空間中的點，若將原點為起點這個點為終點，則可以表示成一個向量；或者可以用矩陣的角度來想，我們可將一筆資料的N個欄位以一個垂直的矩陣表示$\begin{bmatrix} x_1 & x_2 & \cdots & x_n \end{bmatrix}^T$，和向量的矩陣表示方式一模一樣。

馬氏距離的詳細幾何意義解釋可參考這篇[StackExchange](https://stats.stackexchange.com/questions/62092/bottom-to-top-explanation-of-the-mahalanobis-distance)及這篇[博客](http://blog.csdn.net/u010167269/article/details/51627338)

若S為對角矩陣(不同維度之間相關性為零)則可化簡成標準化歐式距離；
若S為單位矩陣(不同維度之間的尺度相等)則可化簡成歐式距離。

以上就是歐式距離及馬氏距離的解釋，其他還有不同的距離計算方式，每種的出發點各異，也都各呈現出不同的資料特性，可以參考最一開始那篇。
